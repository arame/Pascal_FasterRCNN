{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "This code retrieves the trained model, and generates results by running the test image dataset.\n",
    "This code will prints off charts showing the precision/recall curves for each Pascal category.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys, re\n",
    "import time\n",
    "import torch\n",
    "import cv2\n",
    "from pascal_data import PascalVOC2012Dataset\n",
    "from config import Hyper, Constants, OutputStore\n",
    "from utils import load_checkpoint, check_if_target_bbox_degenerate\n",
    "from metrics import compute_ap, compute_class_ap\n",
    "from prediction_buffer import PredictionBuffer\n",
    "from class_data import ClassData\n",
    "from results import save_class_metrics\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(fasterrcnn_model):\n",
    "    OutputStore.check_folder(OutputStore.dir_output_test_images)\n",
    "    start_time = time.strftime('%Y/%m/%d %H:%M:%S')\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{start_time} Starting testing the model\")\n",
    "\n",
    "    class_data = ClassData()\n",
    "    test_dataloader = PascalVOC2012Dataset.get_data_loader(Constants.dir_test_images, \"test\")\n",
    "    fasterrcnn_model.eval()  # Set to eval mode for validation\n",
    "    step = 0\n",
    "    tot_MAP = 0\n",
    "    for id, batch in enumerate(test_dataloader):\n",
    "        _, X, img, img_file, y = batch\n",
    "        step += 1\n",
    "        if step % 100 == 0:\n",
    "            curr_time = time.strftime('%Y/%m/%d %H:%M:%S')\n",
    "            print(f\"-- {curr_time} step: {step}\")\n",
    "        X, y['labels'], y['boxes'] = X.to(Constants.device), y['labels'].to(Constants.device), y['boxes'].to(\n",
    "            Constants.device)\n",
    "        img_file = img_file[0]\n",
    "        img = np.squeeze(img)\n",
    "        # list of images\n",
    "        images = [im for im in X]\n",
    "        targets = []\n",
    "        lab = {'boxes': y['boxes'].squeeze_(0), 'labels': y['labels'].squeeze_(0)}\n",
    "        targets.append(lab)\n",
    "        is_bb_degenerate = check_if_target_bbox_degenerate(targets)\n",
    "        if is_bb_degenerate:\n",
    "            continue  # Ignore images with degenerate bounding boxes\n",
    "        # avoid empty objects\n",
    "        if len(targets) == 0:\n",
    "            continue\n",
    "\n",
    "        # Get the predictions from the trained model\n",
    "        predictions = fasterrcnn_model(images, targets)\n",
    "        # predictions = predictions.to(Constants.model)\n",
    "        # now compare the predictions with the ground truth values in the targets\n",
    "        buffer = PredictionBuffer(targets, predictions)\n",
    "        \n",
    "        MAP, precisions, recalls, overlaps, gt_match, pred_match = compute_ap(buffer)\n",
    "        class_data.add_matches(buffer, gt_match, pred_match)\n",
    "\n",
    "        # MAP, precisions, recalls, overlaps = compute_ap(predictions, targets)\n",
    "        tot_MAP += MAP\n",
    "        output_annotated_images(predictions, img, img_file)\n",
    "        output_stats_for_images(MAP, precisions, recalls, overlaps, img_file)\n",
    "\n",
    "    #output_stats_for_class(MAP, precisions, recalls, overlaps)\n",
    "    print(\"Class level precision and recalls\")\n",
    "    print(\"---------------------------------\")\n",
    "    for i in range(Hyper.num_classes):\n",
    "        class_name = Hyper.pascal_categories[i]\n",
    "        class_gt_match = class_data.gt_match_dict[i]\n",
    "        class_pred_match = class_data.pred_match_dict[i]\n",
    "        if len(class_gt_match) > 0:\n",
    "            MAP, precisions, recalls = compute_class_ap(class_gt_match, class_pred_match)\n",
    "            if MAP == 0:\n",
    "                continue\n",
    "\n",
    "            save_class_metrics(class_name, precisions, recalls)\n",
    "        print(f\"MAP for {class_name}: {MAP}\")\n",
    "    print(\"---------------------------------\\n\\n\")\n",
    "    ave_MAP = tot_MAP / step\n",
    "    print(f\"Average MAP = {ave_MAP}\")\n",
    "    print(\"*** Test run completed ***\")\n",
    "\n",
    "def output_annotated_images(prediction, img, img_file):\n",
    "    boxes = prediction[0][\"boxes\"]\n",
    "    labels = prediction[0][\"labels\"]\n",
    "    scores = torch.round(prediction[0][\"scores\"] * 10) / 10\n",
    "\n",
    "    # this will help us create a different colour for each class\n",
    "    COLOURS = np.random.uniform(0, 255, size=(Hyper.num_classes, 3))\n",
    "    img = cv2.cvtColor(np.array(img), cv2.COLOR_BGR2RGB)\n",
    "    for j in range(2):\n",
    "        for i, box in enumerate(boxes):\n",
    "            label_index = labels[i]\n",
    "            score = str(scores[i].item())[0:3]\n",
    "            if j == 0:\n",
    "                text = f\"{Hyper.pascal_categories[label_index]}\"\n",
    "            else:\n",
    "                text = f\"{Hyper.pascal_categories[label_index]} {score}\"\n",
    "            color = COLOURS[labels[i]]\n",
    "            cv2.rectangle(\n",
    "                img,\n",
    "                (int(box[0]), int(box[1])),\n",
    "                (int(box[2]), int(box[3])),\n",
    "                color, 2\n",
    "            )\n",
    "            cv2.putText(img, text, (int(box[0]), int(box[1] - 5)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1,\n",
    "                        lineType=cv2.LINE_AA)\n",
    "\n",
    "        file_bb = img_file.replace(\".jpg\", f\"_out{j}.jpg\")\n",
    "        path = os.path.join(OutputStore.dir_output_test_images, file_bb)\n",
    "        cv2.imwrite(path, img)\n",
    "\n",
    "\n",
    "def output_stats_for_images(MAP, precisions, recalls, overlaps, img_file):\n",
    "\n",
    "    txt_file = img_file.replace(\"jpg\", \"txt\")\n",
    "    path = os.path.join(OutputStore.dir_output_test_images, txt_file)\n",
    "    lines = [f\"{img_file} metrics\",\n",
    "             \"-\"*20, f\"MAP: {MAP}\",\n",
    "             f\"precisions: {precisions}\",\n",
    "             f\"recalls: {recalls}\",\n",
    "             f\"overlaps: {overlaps}\"]\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write('\\n'.join(lines))\n"
   ]
  },
  {
   "source": [
    "This code below loads the trained model created by running 01main.ipynb and runs the train method above. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = Hyper.total_epochs\n",
    "model, _ = load_checkpoint(epoch)\n",
    "test(model)"
   ]
  }
 ]
}