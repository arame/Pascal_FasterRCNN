{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "This code is used for annotating images taken \"from the wild\" using the trained model.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Hyper, Constants, OutputStore\n",
    "from utils import load_checkpoint\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image as PILImage\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "source": [
    "Code adapted from \n",
    "https://colab.research.google.com/drive/1eAUjzV3nZXkUXi0spPg6zUHtJWaUSzFk?usp=sharing#scrollTo=8Dbx7HZFIKhz \n",
    "\n",
    "I got better results using the PILImage library to convert the image to a tensor.\n",
    "In order to annotate the image I had to use the cv2 library"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def individual_image(fasterrcnn_model, t_image, img, path):\n",
    "    fasterrcnn_model.eval()\n",
    "    prediction = fasterrcnn_model(t_image)\n",
    "    print(prediction)\n",
    "    boxes = prediction[0][\"boxes\"]\n",
    "    labels = prediction[0][\"labels\"]\n",
    "    scores = torch.round(prediction[0][\"scores\"] * 10) / 10\n",
    "\n",
    "    # this will help us create a different colour for each class\n",
    "    COLOURS = np.random.uniform(0, 255, size=(Hyper.num_classes, 3))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    for j in range(2):\n",
    "        for i, box in enumerate(boxes):\n",
    "            label_index = labels[i]\n",
    "            score = str(scores[i].item())[0:3]\n",
    "            if j == 0:\n",
    "                text = f\"{Hyper.pascal_categories[label_index]}\"\n",
    "            else:\n",
    "                text = f\"{Hyper.pascal_categories[label_index]} {score}\"\n",
    "            color = COLOURS[labels[i]]\n",
    "            cv2.rectangle(\n",
    "                img,\n",
    "                (int(box[0]), int(box[1])),\n",
    "                (int(box[2]), int(box[3])),\n",
    "                color, 2\n",
    "            )\n",
    "            cv2.putText(img, text, (int(box[0]), int(box[1]-5)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1,\n",
    "                        lineType=cv2.LINE_AA)\n",
    "\n",
    "        file_bb = path.replace(\".jpg\", f\"_out{j}.jpg\")\n",
    "        print(f\"output file {file_bb}\")\n",
    "        cv2.imwrite(file_bb, img)\n",
    "    print(\"** Images saved, the end **\")\n",
    "\n",
    "\n",
    "# this is same transform used in the dataloader code.\n",
    "def transform_img(img):\n",
    "    t_ = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    img = t_(img)\n",
    "    img = img.unsqueeze_(0)\n",
    "    img = img.to(Constants.device)\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_image(file):\n",
    "    path = os.path.join(Constants.dir_individual_image, file)\n",
    "    img = np.array(PILImage.open(path))\n",
    "    t_img = transform_img(img)\n",
    "    return t_img, img, path\n",
    "\n",
    "\n",
    "def process_images(image_file):\n",
    "    t_image_, img_, path_ = get_image(image_file)\n",
    "    epoch = Hyper.total_epochs\n",
    "    model, _ = load_checkpoint(epoch)\n",
    "    individual_image(model, t_image_, img_, path_)\n"
   ]
  },
  {
   "source": [
    "The next cell runs the code. The images are saved in an output folder set in the config.py file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputStore.check_folder(Constants.dir_individual_image)\n",
    "image_files = [\"aeroplane_with_persons.jpg\", \"cats_and_dogs.jpg\", \"sofa_dog_person.jpg\", \"object_detection.jpg\"]\n",
    "for image_file in image_files:\n",
    "    process_images(image_file)"
   ]
  }
 ]
}